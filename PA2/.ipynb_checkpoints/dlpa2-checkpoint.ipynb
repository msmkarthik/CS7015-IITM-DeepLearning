{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3MMcxogPBxi9"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "np.random.seed(0) \n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "tf.set_random_seed(1)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "import random\n",
    "import scipy\n",
    "import scipy.ndimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "46pBtZ36By5w"
   },
   "outputs": [],
   "source": [
    "def flip_img(img):\n",
    "    index=img.index\n",
    "    img_rs=np.reshape(img.values,(64,64,3))\n",
    "    img_flip=np.fliplr(img_rs)\n",
    "    img_flip=np.reshape(img_flip,-1)\n",
    "    img_flip=pd.Series(img_flip,index=index)\n",
    "    img.update(img_flip)\n",
    "    return(img)\n",
    "\n",
    "def add_salt_pepper_noise(img):\n",
    "    index=img.index\n",
    "    img_rs=np.reshape(img.values,(64,64,3))\n",
    "    \n",
    "    X_imgs_copy = img_rs.copy()\n",
    "    row, col, _ = X_imgs_copy.shape\n",
    "    salt_vs_pepper = 0.3\n",
    "    amount = 0.006\n",
    "    num_salt = np.ceil(amount * X_imgs_copy.size * salt_vs_pepper)\n",
    "    num_pepper = np.ceil(amount * X_imgs_copy.size * (1.0 - salt_vs_pepper))\n",
    "    \n",
    "    coords = [np.random.randint(0, i - 1, int(num_salt)) for i in X_imgs_copy.shape]\n",
    "    X_imgs_copy[coords[0], coords[1], :] = 1\n",
    "\n",
    "    coords = [np.random.randint(0, i - 1, int(num_pepper)) for i in X_imgs_copy.shape]\n",
    "    X_imgs_copy[coords[0], coords[1], :] = 0\n",
    "    \n",
    "    X_imgs_copy=np.reshape(X_imgs_copy,-1)\n",
    "    X_imgs_copy=pd.Series(X_imgs_copy,index=index)\n",
    "    img.update(X_imgs_copy) \n",
    "    return img\n",
    "\n",
    "def rotate_image(img):\n",
    "    index=img.index\n",
    "    img_rs=np.reshape(img.values,(64,64,3))\n",
    "    \n",
    "    angle=np.random.choice([-15,15])\n",
    "    img_rot=scipy.ndimage.rotate(img_rs, angle,reshape=False)\n",
    "    \n",
    "    img_rot=np.reshape(img_rot,-1)\n",
    "    img_rot=pd.Series(img_rot,index=index)\n",
    "    img.update(img_rot) \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_l3dhdQaBzjR"
   },
   "outputs": [],
   "source": [
    "data=pd.read_csv('../data/train.csv')\n",
    "\n",
    "features=data.drop(['id','label'],axis=1)/255\n",
    "features_copy1=features.copy()\n",
    "features_copy2=features.copy()\n",
    "features_copy3=features.copy()\n",
    "label=data['label']\n",
    "labels=label.copy()\n",
    "\n",
    "features_flip=features_copy1.apply(lambda row: flip_img(row),axis=1)\n",
    "#features_noise=features_copy2.apply(lambda row: add_salt_pepper_noise(row),axis=1)\n",
    "features_rotate=features_copy3.apply(lambda row: rotate_image(row),axis=1)\n",
    "features=features.append(features_flip, ignore_index=True)\n",
    "# features=features.append(features_noise, ignore_index=True)\n",
    "features=features.append(features_rotate, ignore_index=True)\n",
    "\n",
    "label=label.append(labels,ignore_index=True)\n",
    "# label=label.append(label,ignore_index=True)\n",
    "label=label.append(labels,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "7TUaLbFDfOE3",
    "outputId": "0be3abda-4b6e-4f84-937e-fef6da9a408e"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# train = pd.read_csv('train.csv',header = 0,index_col = 0).values\n",
    "# X_train = train[:,:-1]/255\n",
    "# train_size = X_train.shape[0]\n",
    "# X_train = X_train.reshape((-1,64,64,3))\n",
    "# y_train = train[:,-1].reshape((-1))\n",
    "# y_train_ohot = np.zeros((train_size,20))\n",
    "# y_train_ohot[range(train_size),y_train.astype(int)] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "val = pd.read_csv('../data/valid.csv',header = 0,index_col = 0).values\n",
    "X_val = val[:,:-1]/255\n",
    "val_size = X_val.shape[0]\n",
    "X_val = X_val.reshape((-1,64,64,3))\n",
    "y_val = val[:,-1].reshape((-1))\n",
    "y_val_ohot = np.zeros((val_size,20))\n",
    "y_val_ohot[range(val_size),y_val.astype(int)] = 1\n",
    "\n",
    "test = pd.read_csv('../data/test.csv',header = 0,index_col = 0)\n",
    "X_test = test.values/255\n",
    "X_test = X_test.reshape((-1,64,64,3))\n",
    "\n",
    "\n",
    "X_train = features.values\n",
    "X_train = X_train.reshape((-1,64,64,3))\n",
    "train_size = X_train.shape[0]\n",
    "\n",
    "y_train = label.values\n",
    "y_train_ohot = np.zeros((train_size,20))\n",
    "y_train_ohot[range(train_size),y_train.astype(int)] = 1\n",
    "\n",
    "print (\"Data done\")\n",
    "\n",
    "\n",
    "print (X_train.shape,y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wk6m5lxBfcj1"
   },
   "outputs": [],
   "source": [
    "def create_mini_batches(X,y,batch_size):\n",
    "\n",
    "    size = X.shape[0]\n",
    "    indices = np.random.permutation(size)\n",
    "    X = X[indices]\n",
    "    y = y[indices]\n",
    "    num_batches = int(size/batch_size)\n",
    "\n",
    "\n",
    "    batches = []\n",
    "    for i in range(num_batches):\n",
    "        X_mini = X[i * batch_size:(i + 1)*batch_size]\n",
    "        y_mini = y[i * batch_size:(i + 1)*batch_size]\n",
    "        batches.append((X_mini, y_mini)) \n",
    "    if (num_batches != size/batch_size):\n",
    "        X_mini = X[num_batches*batch_size:]\n",
    "        y_mini = y[num_batches*batch_size:]\n",
    "        batches.append((X_mini,y_mini))\n",
    "    return batches\n",
    "\n",
    "def create_model(X,is_train):\n",
    "    conv1 = tf.layers.conv2d(inputs = X, filters = 32, kernel_size = [5,5], padding='same',activation = None, strides = [1,1])\n",
    "    bn1 = tf.layers.batch_normalization(inputs = conv1, training = is_train)\n",
    "    act1 = tf.nn.relu(bn1)\n",
    "    \n",
    "    conv2 = tf.layers.conv2d(inputs = act1, filters = 32, kernel_size = [5,5], padding='same',activation = None, strides = [1,1])\n",
    "    bn2 = tf.layers.batch_normalization(inputs = conv2, training = is_train)\n",
    "    act2 = tf.nn.relu(bn2)\n",
    "    \n",
    "    pool1 = tf.layers.max_pooling2d(inputs = act2, pool_size = [2,2], strides = 2)\n",
    "    \n",
    "\n",
    "    conv3 = tf.layers.conv2d(inputs = pool1, filters = 64, kernel_size = [3,3], padding='same',activation = None, strides = [1,1])\n",
    "    bn3 = tf.layers.batch_normalization(inputs = conv3, training = is_train)\n",
    "    act3 = tf.nn.relu(bn3)\n",
    "    \n",
    "    conv4 = tf.layers.conv2d(inputs = act3, filters = 64, kernel_size = [3,3], padding='same',activation = None, strides = [1,1])\n",
    "    bn4 = tf.layers.batch_normalization(inputs = conv4, training = is_train)\n",
    "    act4 = tf.nn.relu(bn4)\n",
    "    \n",
    "    pool2 = tf.layers.max_pooling2d(inputs = act4, pool_size = [2,2], strides = 2)\n",
    "\n",
    "    conv5 = tf.layers.conv2d(inputs = pool2, filters = 64, kernel_size = [3,3], padding='same',activation = None, strides = [1,1])\n",
    "    bn5 = tf.layers.batch_normalization(inputs = conv5, training = is_train)\n",
    "    act5 = tf.nn.relu(bn5)\n",
    "    \n",
    "    conv6 = tf.layers.conv2d(inputs = act5, filters = 128, kernel_size = [3,3], padding='valid',activation = None, strides = [1,1])\n",
    "    bn6 = tf.layers.batch_normalization(inputs = conv6, training = is_train)\n",
    "    act6 = tf.nn.relu(bn6)\n",
    "    \n",
    "    pool3 = tf.layers.max_pooling2d(inputs = act6, pool_size = [2,2], strides = 2)\n",
    "    flatten = tf.reshape(pool3, [-1, 6272])\n",
    "    dp1 = tf.layers.dropout(flatten, rate = 0.3, training = is_train )\n",
    "    \n",
    "    dense1 = tf.layers.dense(inputs = dp1, units = 512, activation = None)\n",
    "    bn7 = tf.layers.batch_normalization(inputs = dense1, training = is_train)\n",
    "    act7 = tf.nn.relu(bn7)\n",
    "    dp2 = tf.layers.dropout(bn7, rate = 0.3, training = is_train )\n",
    "    \n",
    "    dense2 = tf.layers.dense(inputs = dp2, units = 20, activation = None)\n",
    "    bn8 = tf.layers.batch_normalization(inputs = dense2, training = is_train)\n",
    "    logits = tf.nn.softmax(logits = bn8,axis = 1)\n",
    "    return logits\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 454
    },
    "colab_type": "code",
    "id": "RtA4VbjxflVC",
    "outputId": "c6a41c4e-0e7f-446f-eb0e-cd777f624604"
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32, shape = (None,64,64,3))\n",
    "y = tf.placeholder(tf.float32, shape = (None,20))\n",
    "is_train = tf.placeholder(tf.bool)\n",
    "\n",
    "logits = create_model(X,is_train)\n",
    "predictions = tf.argmax(logits, axis=1)\n",
    "loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels = y, logits = logits))\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)                            \n",
    "with tf.control_dependencies(update_ops):                                                            \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate = 0.001).minimize(loss)\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 100\n",
    "patience = 5\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "init_l = tf.local_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "dVmhm_gHfob1",
    "outputId": "6e7117e7-dc00-471d-8859-a15ed5953885"
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run([init,init_l])\n",
    "    prev_val_loss = sess.run(loss, feed_dict = {X : X_val, y: y_val_ohot, is_train : False})\n",
    "    wait_time,i = 0,0\n",
    "    while wait_time < patience and i < epochs:\n",
    "        tot_train_loss = 0\n",
    "        tot_train_acc = 0\n",
    "        batches = create_mini_batches(X_train,y_train_ohot,batch_size)\n",
    "        for k,mini_batch in enumerate(batches):\n",
    "            X_mini, y_mini = mini_batch\n",
    "            _,train_loss,train_preds = sess.run([optimizer,loss,predictions], feed_dict = {X : X_mini, y: y_mini, is_train : True})\n",
    "            tot_train_loss+= 1/(k+1)*(train_loss - tot_train_loss)\n",
    "            tot_train_acc+= 1/(k+1)*(accuracy_score(np.argmax(y_mini,axis=1),train_preds) - tot_train_acc)\n",
    "        new_val_loss, val_preds = sess.run([loss,predictions], feed_dict = {X : X_val, y: y_val_ohot, is_train : False})\n",
    "        if (new_val_loss < prev_val_loss):\n",
    "            wait_time = 0\n",
    "            prev_val_loss = new_val_loss\n",
    "            saver.save(sess,'./temp/model.ckpt')\n",
    "        else:\n",
    "            wait_time+=1\n",
    "        i+=1\n",
    "        print (\"Epoch: {}\".format(i))\n",
    "        print (\"Training loss:\",tot_train_loss,\"Validation loss:\",new_val_loss)\n",
    "        print (\"Training accuracy:\",tot_train_acc,\"Validation accuracy:\",accuracy_score(np.argmax(y_val_ohot,axis=1),val_preds),'\\n')\n",
    "    saver.restore(sess,'./temp/model.ckpt')\n",
    "    val_preds = sess.run(predictions, feed_dict = {X : X_val,is_train : False})\n",
    "    test_predictions = sess.run(predictions, feed_dict = {X : X_test, is_train : False})\n",
    "    print (\"Best validation accuracy:\",accuracy_score(np.argmax(y_val_ohot,axis=1),val_preds))\n",
    "    pd.DataFrame([test.index.astype(int),test_predictions.astype(int)], index = ['id','label']).T.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tuf9vNtEft4q"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_bJH9Nq6fLIJ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "dlpa2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
